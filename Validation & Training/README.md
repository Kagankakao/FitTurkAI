# ğŸ§  FitTÃ¼rkAI AI/ML BileÅŸenleri

<div align="center">

![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.30+-yellow.svg)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-blue.svg)
![QLoRA](https://img.shields.io/badge/QLoRA-PEFT-green.svg)
![Turkish](https://img.shields.io/badge/Language-Turkish%20NLP-red.svg)

*TÃ¼rkÃ§e SaÄŸlÄ±k AI iÃ§in GeliÅŸmiÅŸ Makine Ã–ÄŸrenmesi AltyapÄ±sÄ±*

</div>

---

## ğŸ“‹ Ä°Ã§indekiler

- [ğŸ¯ ModÃ¼ller HakkÄ±nda](#-modÃ¼ller-hakkÄ±nda)
- [ğŸ—ï¸ Sistem Mimarisi](#ï¸-sistem-mimarisi)
- [ğŸ”§ Kurulum ve KonfigÃ¼rasyon](#-kurulum-ve-konfigÃ¼rasyon)
- [ğŸ“ ModÃ¼l DetaylarÄ±](#-modÃ¼l-detaylarÄ±)
- [ğŸš€ KullanÄ±m KÄ±lavuzu](#-kullanÄ±m-kÄ±lavuzu)
- [âš™ï¸ Teknik Parametreler](#ï¸-teknik-parametreler)
- [ğŸ“Š Performans ve Optimizasyon](#-performans-ve-optimizasyon)
- [ğŸ”¬ Algoritma DetaylarÄ±](#-algoritma-detaylarÄ±)
- [ğŸ› Hata Giderme](#-hata-giderme)

---

## ğŸ¯ ModÃ¼ller HakkÄ±nda

Bu klasÃ¶r, **FitTÃ¼rkAI** yapay zeka asistanÄ±nÄ±n Ã§ekirdek makine Ã¶ÄŸrenmesi bileÅŸenlerini iÃ§erir. TEKNOFEST TÃ¼rkÃ§e NLP yarÄ±ÅŸmasÄ± iÃ§in geliÅŸtirilmiÅŸ, model eÄŸitimi ve etkileÅŸim sistemi Ã¼zerine odaklanmÄ±ÅŸ bir AI/ML sistemidir.

### ğŸŒŸ Mevcut Ã–zellikler

- **ğŸ¤– Cosmos Turkish 8B**: YTÃœ'nin TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ LLaMA modeli
- **âš¡ QLoRA Fine-tuning**: Bellek verimli parameter-efficient training
- **ğŸ‡¹ğŸ‡· Turkish NLP**: GeliÅŸmiÅŸ TÃ¼rkÃ§e metin iÅŸleme pipeline'Ä±
- **ğŸ’¬ Interactive Chat**: Terminal tabanlÄ± etkileÅŸimli sohbet sistemi
- **ğŸ¯ Gradio Web ArayÃ¼zÃ¼**: KullanÄ±cÄ± dostu web tabanlÄ± arayÃ¼z

### ğŸš€ Gelecek GeliÅŸtirmeler

- **ğŸ” RAG (Retrieval-Augmented Generation)**: FAISS tabanlÄ± bilgi Ã§ekimi
- **ğŸ“„ Document Processing**: PDF/JSON kaynak entegrasyonu

---

## ğŸ—ï¸ Sistem Mimarisi

```mermaid
graph TB
    subgraph "Mevcut Sistem"
        A[Turkish Text Processor] --> B[Training Data Processing]
        B --> C[Cosmos Turkish 8B]
        C --> D[QLoRA Adapters]
        D --> E[Fine-tuned Model]
        E --> F[Response Generation]
        F --> G[Gradio Web Interface]
        F --> H[Terminal Interface]
    end
    
    subgraph "Gelecek GeliÅŸtirmeler (Roadmap)"
        I[PDF/JSON Processor] -.-> J[Chunking & Embedding]
        J -.-> K[FAISS Index]
        K -.-> L[Document Retrieval]
        L -.-> M[Context Enrichment]
        M -.-> E
    end
    
    style I fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    style J fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    style K fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    style L fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    style M fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
```

### ğŸ”„ Veri AkÄ±ÅŸÄ±

1. **ğŸ“„ Input Processing**: PDF/JSON â†’ Text Chunks
2. **ğŸ” Embedding**: Text â†’ Vector Representations  
3. **ğŸ’¾ Storage**: Vectors â†’ FAISS Index
4. **ğŸ¯ Retrieval**: Query â†’ Relevant Contexts
5. **ğŸ§  Generation**: Context + Query â†’ AI Response
6. **ğŸ–¥ï¸ Interface**: Response â†’ User Display

---

## ğŸ”§ Kurulum ve KonfigÃ¼rasyon

### ğŸ“‹ Sistem Gereksinimleri

| Gereksinim | Minimum | Ã–nerilen | Optimal |
|------------|---------|----------|---------|
| **Python** | 3.8+ | 3.9+ | 3.10+ |
| **RAM** | 8GB | 16GB | 32GB |
| **GPU** | GTX 1060 (6GB) | RTX 3070 (8GB) | RTX 4090 (24GB) |
| **Disk** | 10GB | 20GB | 50GB |
| **CUDA** | 11.0+ | 11.8+ | 12.0+ |

### âš™ï¸ Kurulum AdÄ±mlarÄ±

1. **ğŸ“¦ BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin**
```bash
cd "Validation & Training"
pip install -r requirements.txt
```

2. **ğŸ—„ï¸ NLTK Verilerini Ä°ndirin**
```bash
python -c "
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')  # Yeni versiyon
nltk.download('stopwords')
"
```

3. **ğŸ”§ Environment DeÄŸiÅŸkenleri**
```bash
export CUDA_VISIBLE_DEVICES=0  # GPU seÃ§imi
export TOKENIZERS_PARALLELISM=false  # UyarÄ± Ã¶nleme
```

4. **ğŸ§ª Kurulum Testi**
```bash
python -c "
import torch
from transformers import AutoTokenizer
print(f'PyTorch: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
"
```

---

## ğŸ“ ModÃ¼l DetaylarÄ±

### âœ… Mevcut ModÃ¼ller

#### ğŸ¯ 1. modeltrain.py - Model EÄŸitim Sistemi

#### ğŸ§¬ Ana Fonksiyonlar

| Fonksiyon | AÃ§Ä±klama | Input | Output |
|-----------|----------|-------|--------|
| `ai_model_definition()` | QLoRA konfigÃ¼rasyonlu model yÃ¼kleme | model_name | model, tokenizer |
| `load_and_preprocess_data()` | Veri yÃ¼kleme ve tokenization | data_directory | train/eval datasets |
| `fine_tune_ai_model()` | QLoRA ile fine-tuning | model, datasets | fine-tuned model |
| `fitness_ai_assistant_interaction()` | EtkileÅŸimli test | model, tokenizer | response |

#### ğŸ”§ QLoRA KonfigÃ¼rasyonu

```python
# LoRA Parametreleri
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,                    # LoRA rank
    lora_alpha=32,          # Scaling parameter
    lora_dropout=0.1,       # Dropout rate
    target_modules=[        # Hedef modÃ¼ller
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# 4-bit Quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

#### ğŸ“Š EÄŸitim Parametreleri

```python
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    bf16=True,                    # Mixed precision
    gradient_checkpointing=True,  # Memory optimization
    dataloader_pin_memory=False,
    save_strategy="epoch",
    eval_strategy="epoch"
)
```

#### ğŸ’¬ 2. interaction.py - EtkileÅŸim Sistemi

Bu modÃ¼l, eÄŸitilmiÅŸ model ile terminal tabanlÄ± etkileÅŸimli sohbet imkanÄ± saÄŸlar.

```python
# Basit etkileÅŸimli sohbet
def interactive_chat():
    """Terminal tabanlÄ± sohbet sistemi"""
    model, tokenizer = load_fine_tuned_model()
    
    while True:
        user_input = input("Soru: ")
        response = generate_response(model, tokenizer, user_input)
        print(f"FitTÃ¼rkAI: {response}")
```

#### ğŸ¯ 3. gradio_app.py - Web ArayÃ¼zÃ¼

Gradio tabanlÄ± kullanÄ±cÄ± dostu web arayÃ¼zÃ¼.

```python
import gradio as gr

def chat_interface(message, history):
    """Gradio chat interface"""
    model, tokenizer = load_fine_tuned_model()
    response = generate_response(model, tokenizer, message)
    history.append((message, response))
    return history, ""

# Gradio arayÃ¼zÃ¼
with gr.Blocks() as app:
    gr.Markdown("# ğŸ¥ FitTÃ¼rkAI - TÃ¼rkÃ§e SaÄŸlÄ±k AsistanÄ±")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="SaÄŸlÄ±k sorunuzu yazÄ±n...")
    
app.launch(server_port=7860)
```

### ğŸš€ Gelecek GeliÅŸtirmeler

#### ğŸ” 3. rag_module.py - RAG Sistemi (Planlanan)

Gelecekte geliÅŸtirilecek RAG sistemi ÅŸu Ã¶zellikleri iÃ§erecek:

- **ğŸ“„ Document Processing**: PDF/JSON kaynak iÅŸleme
- **ğŸ” FAISS Vector Store**: Semantik arama sistemi  
- **ğŸ§  Context Integration**: Bilgi Ã§ekimi ve model entegrasyonu
- **ğŸ“š Knowledge Base**: TÃ¼rkÃ§e saÄŸlÄ±k kaynaklarÄ±ndan bilgi Ã§ekimi

---

## ğŸš€ KullanÄ±m KÄ±lavuzu

### âœ… Mevcut KullanÄ±m

#### ğŸ¯ 1. Model EÄŸitimi

```bash
# Temel eÄŸitim
python modeltrain.py

# Custom parametrelerle (gelecekte)
# python modeltrain.py --epochs 5 --batch_size 8 --learning_rate 1e-4
```

**EÄŸitim SÃ¼reci:**
1. Cosmos Turkish 8B modelini yÃ¼kle
2. QLoRA adaptÃ¶rlerini yapÄ±landÄ±r
3. EÄŸitim verisini preprocess et
4. Fine-tuning gerÃ§ekleÅŸtir (3-6 saat)
5. Model ve adaptÃ¶rleri kaydet

#### ğŸ’¬ 2. EtkileÅŸimli Sistemler

```bash
# Terminal tabanlÄ± sohbet
python interaction.py

# Gradio web arayÃ¼zÃ¼
python gradio_app.py
# TarayÄ±cÄ±da: http://localhost:7860
```

**Mevcut Features:**
- ğŸ¤– Fine-tuned Cosmos Turkish 8B model
- ğŸ¯ TÃ¼rkÃ§e saÄŸlÄ±k danÄ±ÅŸmanlÄ±ÄŸÄ±
- ğŸ’¬ Terminal tabanlÄ± etkileÅŸim
- ğŸŒ Gradio web arayÃ¼zÃ¼
- ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e karakter desteÄŸi

### ğŸš€ Gelecek GeliÅŸtirmeler

#### ğŸ” RAG Sistemi (Planlanan)

```bash
# Gelecekte mevcut olacak
# python rag_module.py --pdf_dir ./pdfs --json_dir ../DATA
```

**Planlanan RAG Features:**
- ğŸ“„ PDF/JSON document processing
- ğŸ” FAISS vector search
- ğŸ§  Context-aware responses
- ğŸ“š Knowledge base integration

---

## âš™ï¸ Teknik Parametreler

### ğŸ§  Model Parametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Base Model** | Cosmos Turkish 8B v0.1 | YTÃœ TÃ¼rkÃ§e LLaMA |
| **LoRA Rank** | 16 | Adapter boyutu |
| **LoRA Alpha** | 32 | Scaling faktÃ¶rÃ¼ |
| **Dropout** | 0.1 | Regularization |
| **Max Length** | 2048 | Token limiti |
| **Quantization** | 4-bit nf4 | Bellek optimizasyonu |

### ğŸ” RAG Parametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| **Chunk Size** | 300 words | Metin parÃ§a boyutu |
| **Overlap** | 2 sentences | Chunk Ã¶rtÃ¼ÅŸmesi |
| **Retrieval K** | 5 | Getirilen dokÃ¼man sayÄ±sÄ± |
| **Score Threshold** | 0.2 | Minimum benzerlik skoru |
| **Embedding Model** | MiniLM-L12-v2 | Multilingual encoder |
| **Vector Dimension** | 384 | Embedding boyutu |

### ğŸ”§ Sistem KonfigÃ¼rasyonu

```python
@dataclass
class RAGConfig:
    vector_store_path: str = "./fitness_rag_store_merged"
    chunk_size: int = 300
    chunk_overlap_sentences: int = 2
    retrieval_k: int = 5
    retrieval_score_threshold: float = 0.2
    max_context_length: int = 3000
    embedding_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"
    generator_model_name: str = "ytu-ce-cosmos/Turkish-Llama-8b-v0.1"
    peft_model_path: Optional[str] = "./fine_tuned_FitTurkAI_LoRA"
```

---

## ğŸ“Š Performans ve Optimizasyon

### ğŸ¯ Benchmark SonuÃ§larÄ±

| Metrik | CPU | RTX 3070 | RTX 4090 |
|--------|-----|----------|----------|
| **Inference Speed** | 2-5 tok/s | 25-35 tok/s | 60-80 tok/s |
| **Memory Usage** | 8-12GB | 6-8GB | 4-6GB |
| **RAG Latency** | ~2s | ~0.5s | ~0.3s |
| **Context Window** | 2048 | 2048 | 4096+ |

### âš¡ Optimizasyon Teknikleri

1. **Model Optimizasyonu**
   - 4-bit quantization ile %50 bellek tasarrufu
   - Gradient checkpointing ile bÃ¼yÃ¼k batch'ler
   - Mixed precision (BF16) ile hÄ±z artÄ±ÅŸÄ±

2. **RAG Optimizasyonu**
   - FAISS GPU acceleration
   - Async document processing
   - Smart caching mechanisms

3. **Memory Management**
   - Dynamic batch sizing
   - Automatic garbage collection
   - Smart model loading/unloading

### ğŸ“ˆ Scalability

```python
# BÃ¼yÃ¼k dataset'ler iÃ§in
class ScalableRAG:
    def __init__(self):
        self.distributed_index = faiss.IndexShards()  # Distributed FAISS
        self.async_processor = AsyncDocumentProcessor()
        self.model_parallel = ModelParallel()
```

---

## ğŸ”¬ Algoritma DetaylarÄ±

### ğŸ§® QLoRA AlgoritmasÄ±

```python
def qloï´Œa_forward_pass(x, base_weights, lora_A, lora_B, alpha, scaling):
    """
    QLoRA forward pass implementation
    
    Args:
        x: Input tensor
        base_weights: Frozen 4-bit quantized weights  
        lora_A, lora_B: LoRA adapter matrices
        alpha: LoRA scaling parameter
        scaling: Quantization scaling factor
    """
    # Base model computation (4-bit)
    base_output = quantized_linear(x, base_weights, scaling)
    
    # LoRA adapter computation (full precision)
    lora_output = torch.mm(torch.mm(x, lora_A), lora_B) * (alpha / rank)
    
    return base_output + lora_output
```

### ğŸ” RAG Retrieval AlgoritmasÄ±

```python
def semantic_retrieval(query: str, k: int = 5) -> List[Document]:
    """
    Semantik RAG retrieval algoritmasÄ±
    """
    # 1. Query embedding
    query_vec = sentence_transformer.encode(query)
    
    # 2. FAISS similarity search
    scores, indices = faiss_index.search(query_vec.reshape(1, -1), k)
    
    # 3. Score thresholding
    valid_results = [(doc, score) for doc, score in zip(indices[0], scores[0]) 
                    if score > threshold]
    
    # 4. Context ranking & fusion
    ranked_contexts = rank_by_relevance(valid_results, query)
    
    return ranked_contexts
```

### ğŸ‡¹ğŸ‡· Turkish NLP Pipeline

```python
def turkish_text_pipeline(text: str) -> str:
    """
    TÃ¼rkÃ§e metinler iÃ§in Ã¶zel NLP pipeline'Ä±
    """
    # 1. Character normalization
    text = normalize_turkish_chars(text)
    
    # 2. Sentence segmentation (Turkish-aware)
    sentences = turkish_sent_tokenize(text)
    
    # 3. Stopword removal (Turkish)
    filtered_sentences = remove_turkish_stopwords(sentences)
    
    # 4. Stemming/Lemmatization (Turkish)
    processed_text = turkish_morphological_analysis(filtered_sentences)
    
    return processed_text
```

---

## ğŸ› Hata Giderme

### âŒ YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mleri

#### 1. CUDA Out of Memory

```bash
# Ã‡Ã¶zÃ¼m 1: Batch size kÃ¼Ã§Ã¼ltme
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Ã‡Ã¶zÃ¼m 2: Gradient checkpointing
training_args.gradient_checkpointing = True

# Ã‡Ã¶zÃ¼m 3: Model sharding
model = load_model_sharded(model_name, device_map="auto")
```

#### 2. NLTK Download Errors

```python
# Manuel NLTK setup
import ssl
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download('punkt', quiet=True)
```

#### 3. Turkish Character Issues

```python
# Encoding problemi Ã§Ã¶zÃ¼mÃ¼
def safe_read_turkish(file_path):
    encodings = ['utf-8', 'cp1254', 'iso-8859-9']
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    raise Exception("Turkish character encoding failed")
```

#### 4. FAISS Index Corruption

```python
# Index backup ve recovery
def safe_faiss_operations():
    try:
        # Ana iÅŸlem
        vector_store.build(documents)
        # Backup
        faiss.write_index(index, "backup.index")
    except Exception as e:
        # Recovery
        index = faiss.read_index("backup.index")
        logger.warning(f"Recovered from backup: {e}")
```

### ğŸ”§ Debug ModlarÄ±

```python
# Verbose debugging
import logging
logging.basicConfig(level=logging.DEBUG)

# Memory profiling
import psutil
import GPUtil

def log_system_stats():
    """Sistem kaynaklarÄ±nÄ± logla"""
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    gpus = GPUtil.getGPUs()
    
    logger.info(f"CPU: {cpu_percent}% | RAM: {memory.percent}%")
    for gpu in gpus:
        logger.info(f"GPU {gpu.id}: {gpu.memoryUtil*100:.1f}% | {gpu.temperature}Â°C")
```

### ğŸ“Š Performance Monitoring

```python
# Training metrics tracking
def track_training_metrics():
    """EÄŸitim metriklerini takip et"""
    
    metrics = {
        "loss": [],
        "learning_rate": [],
        "gpu_memory": [],
        "tokens_per_second": []
    }
    
    # Weights & Biases entegrasyonu
    import wandb
    wandb.init(project="fitturkai", config=training_args.__dict__)
```

---

## ğŸš€ GeliÅŸmiÅŸ KullanÄ±m

### ğŸ”„ Custom Training Scripts

```python
# Ã–zelleÅŸtirilmiÅŸ eÄŸitim dÃ¶ngÃ¼sÃ¼
def custom_training_loop():
    """Advanced training configurations"""
    
    # Dynamic learning rate scheduling
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)
    
    # Custom loss functions
    def fitness_specific_loss(outputs, labels):
        """SaÄŸlÄ±k domain'ine Ã¶zel loss"""
        base_loss = F.cross_entropy(outputs.logits, labels)
        health_penalty = calculate_health_safety_penalty(outputs, labels)
        return base_loss + 0.1 * health_penalty
```

### ğŸ¯ Multi-GPU Training

```bash
# DistributedDataParallel ile eÄŸitim
torchrun --nproc_per_node=2 modeltrain.py --distributed

# DeepSpeed entegrasyonu
deepspeed --num_gpus=4 modeltrain.py --deepspeed ds_config.json
```

### ğŸ” Advanced RAG Techniques

```python
# Hybrid retrieval (Dense + Sparse)
class HybridRAG:
    def __init__(self):
        self.dense_retriever = DenseRetriever()  # FAISS
        self.sparse_retriever = SparseRetriever()  # BM25
        self.reranker = CrossEncoder()
    
    def retrieve(self, query: str) -> List[Document]:
        dense_results = self.dense_retriever.search(query)
        sparse_results = self.sparse_retriever.search(query)
        
        # Fusion ve reranking
        combined = self.fusion_algorithm(dense_results, sparse_results)
        reranked = self.reranker.rank(query, combined)
        
        return reranked
```

---

<div align="center">

**ğŸ§  TÃ¼rkiye'nin Ä°lk Yerli SaÄŸlÄ±k AI AltyapÄ±sÄ± ğŸ§ **

*TEKNOFEST 2024 - Yapay Zeka ve Makine Ã–ÄŸrenmesi*

![Made in Turkey](https://img.shields.io/badge/Made%20in-Turkey-red.svg)
![AI/ML](https://img.shields.io/badge/AI%2FML-Advanced-blue.svg)

</div> 